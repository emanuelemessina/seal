\section{Data}
\label{sec:data}

We based our dataset on the Kangxi Dictionary \cite{WikipediaKangxi}, one of the most comprehensive Chinese character dictionaries. It contains over 40000 characters, many of which are obscure or no longer in common use, so we first isolated characters that are included in the Unicode CJK set, reducing the dataset to 16000+ characters.
Next, we identified 18 distinct fonts capable of rendering these characters in a seal script style. We then removed characters that could not be rendered by any of these fonts, leaving 13000+ characters, and further filtered out those represented by fewer than half of the available fonts. This ensured sufficient style variations for each character, ultimately resulting in 5684 characters for our dataset. To ensure a diverse dataset, we incorporated offline data augmentation, avoiding slowdowns during training, since the dataset had to be large anyway, due to the high number of classes. The use of different fonts was actually a type of augmentation itself. We generated synthetic images using 12 different backgrounds, with characters placed in radom sized vertical, square, and single arrangements, mimicking real-world usage in documents and seals. Each character underwent a random perspective deformation, and 2D rotation for added variation. Images were sized at 256×256 pixels, containing 1 to 7 characters per image.
To estimate the number of images required, we used the coupon collector’s problem \cite{CouponCollectorProblem}, determining the necessary number of random draws to ensure all characters appeared at least once.
Given an average of 3.5 characters per image, we rounded up our calculations to 35000 generated images. Since both the dataset and model were experimental, we prioritized convergence over extensive validation. We performed a simple $90\%-10\%$ split, generating 4,000 test images.
For labeling, we created a custom annotation format tailored to our hierarchical model. Additionally, we generated a YOLO-compatible label set, which does not incorporate hierarchical classification. We computed the mean and standard deviation of the dataset to normalize inputs during training. Lastly, we analyzed the distribution of characters and radicals, finding a strong bias toward common radicals (\ref{img:radicals_distribution}). To mitigate this imbalance, we calculated class weights for both radicals and individual characters, which were later used in a weighted cross-entropy loss function during training, according to the following formula \cite{WeightsFormulaArticle}:
\[
    w_c = \frac{\sum_{i \in C, s \in S}{n^s_i}}{|C| n_c}
\]
where $w_c$ is the weight assigned to class $c$, $C$ is the classes set and $|C|$ is the number of classes, $n_c$ is the absolute frequency of class $c$ over the whole dataset, $S$ is the dataset and $n^s_i$ is the frequency of class $i$ in sample $s$ (meaning the numerator is the total number of instances).

\img{radicals_distribution}{\linewidth}{Radicals distribution over the train dataset. The dataset contains all 214 of them.}